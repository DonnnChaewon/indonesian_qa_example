{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5234d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles were scraped\n"
     ]
    }
   ],
   "source": [
    "def scrape_cnn_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Ambil konten halaman indeks\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        news_links = []\n",
    "        \n",
    "        # Temukan semua link berita di halaman indeks - pendekatan yang lebih spesifik\n",
    "        article_containers = soup.find_all(['article', 'div'], class_=re.compile(r'(article|news|berita)'))\n",
    "        \n",
    "        for container in article_containers:\n",
    "            link_element = container.find('a', href=True)\n",
    "            if link_element:\n",
    "                link = link_element['href']\n",
    "                # Pastikan link lengkap (bukan relative URL)\n",
    "                if link.startswith('/'):\n",
    "                    link = 'https://www.cnnindonesia.com' + link\n",
    "                if '/nasional/' in link and link not in news_links:\n",
    "                    news_links.append(link)\n",
    "        \n",
    "        # Persiapan untuk menyimpan data\n",
    "        news_data = []\n",
    "        \n",
    "        # Scraping setiap berita individu\n",
    "        for news_url in news_links:\n",
    "            try:\n",
    "                # Delay untuk menghindari request berlebihan\n",
    "                time.sleep(3)\n",
    "                \n",
    "                news_response = requests.get(news_url, headers=headers)\n",
    "                news_response.raise_for_status()\n",
    "                \n",
    "                news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "                \n",
    "                # Ekstrak judul - beberapa kemungkinan lokasi\n",
    "                title = None\n",
    "                title_selectors = ['h1', 'h2', ['meta', 'property', 'og:title']]\n",
    "                \n",
    "                for selector in title_selectors:\n",
    "                    if isinstance(selector, list) and selector[0] == 'meta':\n",
    "                        meta_tag = news_soup.find('meta', {selector[1]: selector[2]})\n",
    "                        if meta_tag and meta_tag.get('content'):\n",
    "                            title = meta_tag['content']\n",
    "                            break\n",
    "                    else:\n",
    "                        title_tag = news_soup.find(selector)\n",
    "                        if title_tag:\n",
    "                            title = title_tag.get_text(strip=True)\n",
    "                            if title:\n",
    "                                break\n",
    "                \n",
    "                if not title:\n",
    "                    title = 'No Title'\n",
    "                \n",
    "                # Ekstrak konten berita - beberapa kemungkinan lokasi\n",
    "                content = None\n",
    "                content_selectors = [\n",
    "                    'div.detail__body-text',\n",
    "                    'div.text-detail',\n",
    "                    'div.article-content',\n",
    "                    'div.content'\n",
    "                ]\n",
    "                \n",
    "                for selector in content_selectors:\n",
    "                    content_div = news_soup.select_one(selector)\n",
    "                    if content_div:\n",
    "                        # Hapus tag yang tidak diinginkan dari konten\n",
    "                        for element in content_div.find_all(['script', 'style', 'div.ads', 'figure', 'iframe', 'ul.related']):\n",
    "                            element.decompose()\n",
    "                        \n",
    "                        # Ambil semua paragraf\n",
    "                        paragraphs = content_div.find_all('p')\n",
    "                        if paragraphs:\n",
    "                            content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "                            content = re.sub(r'\\s+', ' ', content).strip()\n",
    "                            break\n",
    "                \n",
    "                if not content:\n",
    "                    content = 'No Content'\n",
    "                \n",
    "                # Ekstrak tanggal\n",
    "                date = None\n",
    "                date_selectors = [\n",
    "                    'div.date',\n",
    "                    'span.date',\n",
    "                    'time',\n",
    "                    ['meta', 'property', 'article:published_time']\n",
    "                ]\n",
    "                \n",
    "                for selector in date_selectors:\n",
    "                    if isinstance(selector, list) and selector[0] == 'meta':\n",
    "                        meta_tag = news_soup.find('meta', {selector[1]: selector[2]})\n",
    "                        if meta_tag and meta_tag.get('content'):\n",
    "                            date = meta_tag['content']\n",
    "                            break\n",
    "                    else:\n",
    "                        date_tag = news_soup.find(selector)\n",
    "                        if date_tag:\n",
    "                            date = date_tag.get_text(strip=True)\n",
    "                            if date:\n",
    "                                break\n",
    "                \n",
    "                if not date:\n",
    "                    date = 'No Date'\n",
    "                \n",
    "                # Simpan data\n",
    "                news_data.append({\n",
    "                    'title': title,\n",
    "                    'content': content,\n",
    "                    'date': date,\n",
    "                    'url': news_url\n",
    "                })\n",
    "                \n",
    "                print(f\"Scraped: {title}\")\n",
    "                print(f\"Content length: {len(content)} characters\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {news_url}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Simpan ke CSV\n",
    "        if news_data:\n",
    "            df = pd.DataFrame(news_data)\n",
    "            df.to_csv('cnn_nasional_1.csv', index=False, encoding='utf-8-sig')\n",
    "            print(f\"Successfully saved {len(news_data)} articles\")\n",
    "        else:\n",
    "            print(\"No articles were scraped\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# URL target\n",
    "target_url = \"https://www.cnnindonesia.com/nasional/indeks/3\"\n",
    "scrape_cnn_news(target_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
